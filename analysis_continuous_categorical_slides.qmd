---
title: "Analysis of Variance"
subtitle: "Testing Equality of Means"
author: Rodney Dyer, PhD
format: revealjs
execute:
  echo: true
---


## Testing for Specific Values

In general, many times we are interested in testing to see if some samples have a particular value.  Examples may include:

- $H_O: \mu = c$  The mean of the data is equal to a specific value.  
- $H_O: \mu_1 = \mu_2$  These two samples have the same mean.  
- $H_O: \mu_1 = \mu_2 = \mu_3 ... \mu_k$: The mean of all $k$ values are all the same.





## 1-Sample Tests

The 1-sample `t.test()` defined by the null hypothesis $H_O: \mu = c$ is where we are testing to see if this sample 

$t =\frac{\bar{x}-\mu}{s_{\bar{x}}}$








## Degrees of Freedom & Rejection Regions

```{r}
#| echo: false
library( tidyverse )
ggplot2::theme_set( theme_minimal() )
x <- seq(-5,5,by=0.02)
d <- data.frame( t=c(x,x,x),
                  f=c(dt(x,df=1),
                      dt(x,df=3),
                      dt(x,df=Inf)),
                  df=rep(c("1","3","Inf"),each=length(x)))

ggplot( d, aes(x=t,y=f,color=df)) + geom_line() 
```


## Asymptotic Nature of Samples

As $df \to \infty$ then $PDF(t) \to Normal$

&nbsp;

Which is defined (in such elegance) as:

&nbsp;

$P(t|x,v)= \frac{ \Gamma\left( \frac{v+1}{2}\right)}{\sqrt{v\pi}\Gamma\left( \frac{v}{2}\right)} \left( 1 + \frac{x^2}{v}\right)^{-\frac{v+1}{2}}$




## Rejection Regions
:::: {.columns}

::: {.column width="50%"}


Two Tailed Test: $H_A: \mu \ne c$

One Tailed Test: $H_A: \mu \lt c$

One Tailed Test: $H_A: \mu \gt c$

:::

::: {.column width="50%"}
```{r echo=FALSE}
d1 <- data.frame(t=c( seq(-5,-2.064, by=0.02), -2.064, -5), 
                 f=c( dt( seq(-5,-2.064, by=0.02),df=1), 0.01224269, 0.01224269))
d2 <- data.frame(t=c( seq(2.064,5,by=0.02), 5, 2.064),
                 f=c( dt( seq( 2.064, 5, by=0.02),df=1), 0.01224269, 0.01224269))
d3 <- data.frame( x=c(2.5,-2.5), y=0.02719, label="2.5%")
ggplot() + 
  geom_polygon(aes(t,f),data=d1, fill="#F8766D",alpha=0.5,color="#F8766D") + 
  geom_polygon(aes(t,f),data=d2, fill="#F8766D",alpha=0.5,color="#F8766D") + 
  geom_line( aes(t,f),data=d[d$df==1,], color="#F8766D") + 
  geom_text( aes(x,y,label=label),data=d3)
```
:::

::::



# One Sample Test {background-color="black" background-image="media/contour.png" background-size="initial" background-position="right"}



## The Iris Data (Again) {.smaller}

:::: {.columns}

::: {.column width="50%"}
```{r}
#| echo: false
library( knitr )
iris %>%
  group_by( Species ) %>%
  summarize( SL.m = mean( Sepal.Length),
             SL.sd = sd( Sepal.Length ) ) %>%
  mutate( `Sepal Length` = paste( SL.m, "+/-", format( SL.sd, digits=3) , sep=" " ) ) %>%
  mutate( Species = paste( "I.",Species) ) %>%
  select( Species, `Sepal Length` ) %>%
  rbind(tibble( Species="Average", `Sepal Length` = "5.843")) %>%
  kable( digits=3)
```
:::

::: {.column width="50%"}
![The enigmatic *Iris*.](https://live.staticflickr.com/65535/50609360207_92d21b056f_w_d.jpg)
:::

::::




## Iris setosa Example {.smaller}

$H_O: Iris\;setosa\;Sepal\;Length = 5.843$

:::: {.columns}

::: {.column width="50%"}


```{r}
#| echo: true
iris %>%
  filter( Species == "setosa" ) %>%
  select( Sepal.Length ) %>%
  t.test( mu=5.843 )
```
:::

::: {.column width="50%"}
```{r}
#| echo: false 
ggplot( iris, aes(x=Sepal.Length, fill=Species)) + geom_density(alpha=0.75)
```
:::

::::



## Iris versicolor Example {.smaller}

$H_O: Iris\;versicolor\;Sepal\;Length = 5.843$

:::: {.columns}

::: {.column width="50%"}

```{r}
#| echo: true
iris %>%
  filter( Species == "versicolor" ) %>%
  select( Sepal.Length ) %>%
  t.test( mu=5.843 )
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
ggplot( iris, aes(x=Sepal.Length, fill=Species)) + geom_density(alpha=0.75)
```
:::

::::



## Iris virginica Example {.smaller}

$H_O: Iris\;virginica\;Sepal\;Length = 5.843$

:::: {.columns}

::: {.column width="50%"}



```{r}
#| echo: true
iris %>%
  filter( Species == "virginica" ) %>%
  select( Sepal.Length ) %>%
  t.test( mu=5.843 )
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
ggplot( iris, aes(x=Sepal.Length, fill=Species)) + geom_density(alpha=0.75)
```
:::

::::


## Confidence Intervals.


```{r}
#| echo: true
t <- iris %>%
  filter( Species == "virginica" ) %>%
  select( Sepal.Length ) %>%
  t.test( mu=6 ) 
names(t)
```


:::{.fragment}
The confidence interval itself is defined by the underlying t-distribution as:

$\bar{x} - t_{\alpha, df} s_{\bar{x}} < \mu < \bar{x} + t_{\alpha, df} s_{\bar{x}}$

```{r}
t$conf.int
```
:::



# Two-Sample Tests {background-color="black" background-image="media/contour.png" background-size="initial" background-position="right"}


## Evaluating Equality of 2 Means

:::: {.columns}

::: {.column width="50%"}
### Null Hypothesis

$H_O: \mu_{versicolor} = \mu_{virginica}$

Which is estimated by the statistic.

$t = \frac{\bar{x}_1 - \bar{x}_2}{s_{\bar{x}_1-\bar{x}_2}}$

where $s_{\bar{x}_1-\bar{x}_2}$ is referred to as the *pooled variance* and is defined as:

$s_{\bar{x}_1-\bar{x}_2} = \sqrt{ \frac{s_1^2}{N_1}+\frac{s_2^2}{N}}$

:::

::: {.column width="50%"}
```{r}
#| echo: false 
iris %>%
  filter( Species != "setosa" ) %>%
  ggplot( aes(Species,Sepal.Length) ) + geom_boxplot(notch=TRUE)
```
:::

::::



## Testing Equality of 2 Means

```{r}
x <- iris %>% 
  filter( Species == "versicolor") %>% 
  select( Sepal.Length ) 
y <- iris %>% 
  filter( Species == "virginica") %>% 
  select( Sepal.Length ) 
t.test( x, y )
```

## Paired t-tests

If the data are collected in a way that there is supposed to be a *connection* between the values of `x` and `y` (e.g., looking at fluctuating asymeetry in wing shape in individual birds where left wing = right wing), you could instead treat the two data as:

$H_O: \mu_{x} -  \mu_{y} = 0.0$

and can be evaluated as:

```{r eval=FALSE}
t.test(x, y, paired = TRUE)
```


:::{.fragment}
These data are not `paired` so I will not provide an example.  The key here is that the data must come from a biologically and/or ecologically relevant pairing.
:::


# Equality of Many Means {background-color="black" background-image="media/contour.png" background-size="initial" background-position="right"}


## Why Not All-Pairs t-tests? {.smaller}

$H_O: Not\;Pregnant$

:::{.fragment}
![](https://live.staticflickr.com/65535/53362227720_27627d3ec4_o_d.png)
:::

:::: {.columns}

::: {.column width="50%"}
Rejecting of a true null hypothesis `false positive`
:::

::: {.column width="50%"}
Non-rejection of a false null hypothesis, `false negative`
:::

::::


## Multiple Testing Problem

The rejection region as $\alpha$ is defined as the fraction of the underlying distribution that we will consider as rare enough to reject $H_O$.  

&nbsp;

The probability of rejecting $H_O$ incorrectly is $\alpha$ and the probability of not rejecting $H_O$ incorrectly is $1-\alpha$.  


## Multiple Testing Problem

> What is the probability of getting at least one false positive result if we test all three pairs of individual mean values in the Iris data set?

:::{.fragment}

$P(false\;positive) = 1 - P(no\;significant\;results)$ 

for each trial.  So, if we are to do 3 tests, this is equal to 

$1 - (1-0.05)^3 \approx 0.1426$
:::

## Multiple Testing Problem

If we test all three pairs of tests for the *Iris* data, what mean that we are expecting, <font color="red">even if there are absolutely **no differences** between species</font>, to reject the $H_O$ at least 14% of the time!


:::{.fragment}
&nbsp;

**Change either $\alpha$ (known as a Bonferroni Correction) or adjust the Model itself (preferred).**
:::


## A Linear Model Approach 

```{r}
#| echo: false
ggplot( iris, aes(x=Species, y=Sepal.Length)) + 
  geom_boxplot(notch = TRUE, color="#cc8011") + 
  stat_summary( color="#0F8B8D", size=1.3 ) +
  geom_point( alpha=0.75) + 
  geom_jitter( width=0.25) +
  ylab("Sepal Length") 
```

$y_{ij} = \mu + \tau_i + \epsilon_j$


## A Linear Model Approach {.scrollable}


$y_{ij} = \mu + \tau_i + \epsilon_j$

where:   

- $\mu$ is the overall mean of all the data.  
- $\tau_i$ is the deviation of the $i^{th}$ treatment level from the overall mean.  
- $\epsilon_j$ is the deviation of the observation from the treatment mean.



:::{.fragment}
*The Null Hypothesis*
$H_O: \tau_i = 0$
:::


## Decomposing the Variance

The Analysis of Variance Table for Testing the Null Hypothesis $H_O: \tau_i = 0$.

&nbsp;

Source  | df    | SS                                                                 | MS
----|---|----------------------------------------------------------------|------------------------------------------------
Among   | $K-1$ | $\sum_{i=1}^K N_i \left( \bar{x}_i - \bar{x} \right)^2$            | $\frac{SS_A}{K-1}$
Within  | $N-K$ | $\sum_{i=1}^Kn_i\left( \sum_{j=1}^{N_i}(x_{ij}-\bar{x}_i)^2 \right)$ | $\frac{SS_W}{N-K}$
Total   | $N-1$ | $\sum_{i=1}^K \sum_{j=1}^{N_i} (x_{ij} - \bar{x})^2$               | 


## The Test Statistic

Which is evaluated by the statistic:

$F = \frac{MS_A}{MS_W}$

::: {.fragment}
defined by the most excellent $F$ distribution.
&nbsp;
$f(x | df_A, df_W) = \frac{\sqrt{\frac{(df_Ax)^{df_A}df_W^{df_W}}{(df_Ax + df_W)^{df_W+df_A}}}}{x\mathbf{B}\left( \frac{df_A}{2}, \frac{df_W}{2} \right)}$
:::



## Analysis of Variance (aov)

In `R` we use the `aov()` function.

```{r}
fit.aov <- aov( Sepal.Length ~ Species, data=iris)
fit.aov
```

## Analysis of Variance Table

The variance decomposition for this model, just like for regression models, can be displayed using the `anova()` summary function.

```{r}
anova( fit.aov )
```





# Post-Hoc Testing {background-color="black" background-image="media/contour.png" background-size="initial" background-position="right"}



## Why Post-Hoc?

Potential Outcomes where $H_O$ is rejected:

- $\mu_{setosa} \; \ne \mu_{virginica}$

- $\mu_{setosa} \; \ne \mu_{versicolor}$

- $\mu_{virginica} \; \ne \mu_{versicolor}$

- $\mu_{setosa} \; \ne \; \mu_{virginica} \; \ne \mu_{versicolor}$


:::{.fragment}
By rejecting $H_O$, we do not know <font color="red">which of the previous statements</font> caused the Among-Treatment Sums of Squares to be large enough to push the estimate of $F$ into the rejection region of the distribution.
:::


## The Tukey Test

This test examines all pairs of means and determines if their confidence intervals overlap or not.

```{r}
tuk <- TukeyHSD(fit.aov)
tuk
```


## The Tukey Test

The difference is: 

$q = \frac{\bar{y}_{max} - \bar{y}_{min}}{\sigma \sqrt{2/N}}$

and the confidence intervals are based upon Student-ized confidence ranges.






## Visualizing the Tukey

```{r fig.height= 5.5}
plot( tuk, xlim=c(-1,2) )
```








# Two-Factor Models {background-color="black" background-image="media/contour.png" background-size="initial" background-position="right"}




## Analysis with 2 Predictors.

There are times when we have more than on treatment category for a response.  For example, a growth experiment may have a treatment that is high light and low light as well as one that has fewer than normal, normal, and more than normal nutrients.  These are called *2-factor Analysis of Variance*.

- Each factor can be considered independently.
- The interaction between factors can be exmained as well.

## Specifying A Two Factor Model {.smaller}

Here i'm going to use a new data set because there is only one categorical variable in the *iris* data used above.  Here is the old *Motor Trend Cars* dataset

```{r}
df <- mtcars
summary( df )
```

## Quarter Mile Speed ~ Engine Type & Transmission

```{r}
df %>% 
  mutate( Engine = factor( ifelse( vs == 0, "V-Shaped", "Straight") ),
          Transmission = factor( ifelse( am == 0, "Automatic","Manual")) ) %>%
  select( Time = qsec, Engine, Transmission) -> df 
summary( df )
```

## One Factor Analyses: Engine


```{r}
fit.engine <- aov( Time ~ Engine, data=df )
anova(fit.engine)
```

## One Factor Analyses: Transmission


```{r}
fit.transmission <- aov( Time ~ Transmission, data=df )
anova( fit.transmission )
```




## Interaction Terms

This notation inserts both terms independently **and** their interaction into the model.

```{r}
fit.both <- aov( Time ~ Engine*Transmission, data=df )
anova( fit.both )
```

## &nbsp;

```{r}
plot( TukeyHSD( fit.both, "Engine"))
```

## &nbsp;

```{r}
plot( TukeyHSD( fit.both, "Transmission"))
```

## &nbsp;

```{r}
plot( TukeyHSD( fit.both, "Engine:Transmission"))
```














## Questions

::: {layout-ncol="2"}
If you have any questions, please feel free to either post them as an "Issue" on your copy of this GitHub Repository, post to the [Canvas](https://canvas.vcu.edu) discussion board for the class, or drop me an [email](mailto://rjdyer@vcu.edu).

![](media/peter_sellers.gif){.middle fig-alt="Peter Sellers looking bored" fig-align="center" width="500"}
:::
